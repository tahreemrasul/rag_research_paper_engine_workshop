{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Research Paper Engine using arXiv, LangChain ğŸ¦œï¸ğŸ”— and Google Gemini"
      ],
      "metadata": {
        "id": "PetNuzxJZwow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Tahreem Rasul](https://github.com/tahreemrasul) |"
      ],
      "metadata": {
        "id": "xvIyW-EMukRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/tahreemrasul/rag_research_paper_engine_workshop/blob/main/rag_research_paper_engine.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/tahreemrasul/rag_research_paper_engine_workshop/blob/main/rag_research_paper_engine.ipynb\">\n",
        "      <img width=\"28px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "w82RyFPUG4W-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates implementing a research paper engine using the arXiv API to show how to improve LLM's response by augmenting LLM's knowledge with external data sources such as documents. The notebooks uses Vertex AI Gemini Pro 1.0 for Text, Embeddings for Text API, arXiv API and LangChain ğŸ¦œï¸ğŸ”—.\n",
        "\n",
        "## Context\n",
        "\n",
        "Large Language Models (LLMs) have improved quantitatively and qualitatively. They can learn new abilities without being directly trained on them. However, there are constraints with LLMs - they are unaware of events after training and it is almost impossible to trace the sources to their responses. It is preferred for LLM based systems to cite their sources and be grounded in facts.\n",
        "\n",
        "To solve for the constraints, one of the approaches is to augment the prompt sent to LLM with relevant data retrieved from an external knowledge base through Information Retrieval (IR) mechanism.\n",
        "\n",
        "This approach is called Retrieval Augmented Generation (RAG), also known as Generative QA in the context of a Question Answering task. There are two main components in RAG based architecture: (1) Retriever and (2) Generator."
      ],
      "metadata": {
        "id": "S-KL6H18sXA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "### Install Vertex AI SDK, other packages and their dependencies\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ],
      "metadata": {
        "id": "9dVwHu5gs-dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LangChain and related packages\n",
        "!pip install --upgrade --quiet langchain langchain-google-vertexai langchain-community chromadb arxiv pymupdf chainlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqZuoY_dZ7G8",
        "outputId": "134247df-0d08-4d0a-d3d2-44ba03104ee2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for literalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for syncer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ],
      "metadata": {
        "id": "SKByjcaPtDHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgtqfYu9jf7o",
        "outputId": "de7c7b66-9b61-4cc6-ff60-858c5c5edf6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>âš ï¸ Before proceeding, please wait for the kernel to finish restarting âš ï¸</b>\n",
        "</div>"
      ],
      "metadata": {
        "id": "v2cbF6nptJu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticating your notebook environment\n",
        "\n",
        "If you are using Colab, you will need to authenticate yourself first. The next cell will check if you are currently using Colab, and will start the authentication process.\n",
        "\n",
        "If you are using Vertex AI Workbench, you will not require additional authentication.\n",
        "\n",
        "For more information, you can check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
      ],
      "metadata": {
        "id": "V2-PTrJjuHVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "vPo0ch83ounD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Relevant Papers from arXiv API\n",
        "\n",
        "This step retrieves relevant research papers based on the user query. The document corpus used as dataset will be the research papers pulled from the `arXiv` API. We will be using the `ArxivLoader` class from LangChain to load the PDFs of these papers."
      ],
      "metadata": {
        "id": "wFJUl7RhpYdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query & No. of Papers { display-mode: \"form\" }\n",
        "query = \"neural networks\"  # @param {type:\"string\"}\n",
        "\n",
        "# @title Total Docs { display-mode: \"form\" }\n",
        "num_papers = \"3\"  # @param {type: \"string\"}"
      ],
      "metadata": {
        "id": "JsTU3RZkx7dd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import ArxivLoader\n",
        "\n",
        "arxiv_docs = ArxivLoader(query=query, load_max_docs=int(num_papers)).load()"
      ],
      "metadata": {
        "id": "tOBHwVySxepN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once retreived, display the metadata to check which papers were returned"
      ],
      "metadata": {
        "id": "2661AqkeqwlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(int(num_papers)):\n",
        "  print(f\"Paper # {i+1}:\")\n",
        "  print(f\"Published: {arxiv_docs[i].metadata['Published']}\")\n",
        "  print(f\"Title: {arxiv_docs[i].metadata['Title']}\")\n",
        "  print(f\"Authors: {arxiv_docs[i].metadata['Authors']}\")\n",
        "  print(f\"Summary: {arxiv_docs[i].metadata['Summary']}\")\n",
        "  print('------------------------------------------------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPOwT8JRy3Ww",
        "outputId": "fb4c0a12-5fb5-4f20-fd47-56b3d9138032",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper # 1:\n",
            "Published: 2023-04-18\n",
            "Title: Lecture Notes: Neural Network Architectures\n",
            "Authors: Evelyn Herberg\n",
            "Summary: These lecture notes provide an overview of Neural Network architectures from\n",
            "a mathematical point of view. Especially, Machine Learning with Neural Networks\n",
            "is seen as an optimization problem. Covered are an introduction to Neural\n",
            "Networks and the following architectures: Feedforward Neural Network,\n",
            "Convolutional Neural Network, ResNet, and Recurrent Neural Network.\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "Paper # 2:\n",
            "Published: 2023-11-18\n",
            "Title: Bayesian Neural Networks: A Min-Max Game Framework\n",
            "Authors: Junping Hong, Ercan Engin Kuruoglu\n",
            "Summary: Bayesian neural networks use random variables to describe the neural networks\n",
            "rather than deterministic neural networks and are mostly trained by variational\n",
            "inference which updates the mean and variance at the same time. Here, we\n",
            "formulate the Bayesian neural networks as a minimax game problem. We do the\n",
            "experiments on the MNIST data set and the primary result is comparable to the\n",
            "existing closed-loop transcription neural network. Finally, we reveal the\n",
            "connections between Bayesian neural networks and closed-loop transcription\n",
            "neural networks, and show our framework is rather practical, and provide\n",
            "another view of Bayesian neural networks.\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "Paper # 3:\n",
            "Published: 2005-04-13\n",
            "Title: Self-Organizing Multilayered Neural Networks of Optimal Complexity\n",
            "Authors: V. Schetinin\n",
            "Summary: The principles of self-organizing the neural networks of optimal complexity\n",
            "is considered under the unrepresentative learning set. The method of\n",
            "self-organizing the multi-layered neural networks is offered and used to train\n",
            "the logical neural networks which were applied to the medical diagnostics.\n",
            "------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunk documents - TextSplitter\n",
        "\n",
        "Split the documents retrieved into smaller chunks. When splitting the document, ensure a few chunks can fit within the context length of LLM."
      ],
      "metadata": {
        "id": "mCBqPg7hq0ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "pdf_data = []\n",
        "for doc in arxiv_docs:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    doc_splits = text_splitter.create_documents([doc.page_content])\n",
        "    for idx, split in enumerate(doc_splits):\n",
        "      split.metadata[\"chunk\"] = idx\n",
        "    pdf_data.append(doc_splits)\n",
        "\n",
        "print(f\"# of pdfs = {len(pdf_data)} \\n# of split documents = {sum([len(doc_splits) for doc_splits in pdf_data])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5IDeM1g_lpc",
        "outputId": "7c00f142-b8c1-4aff-9b63-09da553c9f86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of pdfs = 3 \n",
            "# of split documents = 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the VertexAI Embedding model"
      ],
      "metadata": {
        "id": "w21N8p80rqKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Project { display-mode: \"form\" }\n",
        "PROJECT_ID = \"research-paper-engine\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "\n",
        "# @title Region { display-mode: \"form\" }\n",
        "REGION = \"us-central-1\"  # @param {type: \"string\"}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf8_Q7mApLmM",
        "outputId": "2a6f8078-23ec-4001-bb80-05c8bd4a3947"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "embedding_model = VertexAIEmbeddings(\n",
        "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
        ")\n",
        "print(embedding_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68qVOQKyuQoT",
        "outputId": "8b17c696-8edc-46a3-af2b-d457d5631fe6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=<vertexai.language_models.TextEmbeddingModel object at 0x7aa595b93280> async_client=None project='research-paper-engine' location='us-central1' request_parallelism=5 max_retries=6 stop=None model_name='textembedding-gecko@latest' model_family=None full_model_name=None client_preview=None temperature=None max_output_tokens=None top_p=None top_k=None credentials=None n=1 streaming=False safety_settings=None api_transport=None api_endpoint=None tuned_model_name=None instance={'max_batch_size': 250, 'batch_size': 250, 'min_batch_size': 5, 'min_good_batch_size': 5, 'lock': <unlocked _thread.lock object at 0x7aa5cceb4100>, 'batch_size_validated': False, 'task_executor': <concurrent.futures.thread.ThreadPoolExecutor object at 0x7aa5cce6fbe0>, 'embeddings_task_type_supported': True, 'get_embeddings_with_retry': <function _TextEmbeddingModel.get_embeddings at 0x7aa595bdaa70>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `ChromaDB` as Vector Store\n",
        "\n",
        "This step generates embeddings from the documents and adds the embeddings to the vector store. The vector store being used is the `Chroma` database.\n"
      ],
      "metadata": {
        "id": "I_5nT5YVozTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "db = Chroma.from_documents(pdf_data[0], embedding_model)"
      ],
      "metadata": {
        "id": "lqyY4z2STxNz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title search query { display-mode: \"form\" }\n",
        "search_query = \"What should be considered when taking derivatives of ReLU?\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "skgUgy7kga3N"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify the `ChromaDB` with similarity search"
      ],
      "metadata": {
        "id": "r_fuacRXpPV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db.similarity_search(\n",
        "    search_query\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S48Gk44We3rI",
        "outputId": "2d0387eb-5e58-457b-e899-184c7dbf2809"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='identity function often helps speed up convergence, since it resembles a linear model, as long as\\nthe values are close to zero. Another challenge that needs to be overcome is vanishing derivatives,\\nwhich is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not\\nbounded on positive values, while also being comparatively cheap to compute, because linear\\ncomputations tend to be very well optimized in modern computing. Altogether, these advan-\\ntages have resulted in ReLU (and variants thereof) becoming the most widely used activation\\nfunction currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was\\nintroduced. When taking derivatives of ReLU one needs to account for the non-diï¬€erentiability\\nat 0, but in numerical practice this is easily overcome.\\nWith the help of Neural Networks we want to solve a task, cf.\\n[15, Section 5.1].\\nLet the\\nperformance of the algorithm for the given task be measured by the loss function L, which', metadata={'chunk': 8}),\n",
              " Document(page_content='â‡’\\ny[â„“] âˆ’y[â„“âˆ’1]\\nÏ„ [â„“]\\n= Ïƒ(W [â„“âˆ’1]y[â„“âˆ’1] + b[â„“âˆ’1]).\\n4\\nRESNET\\n38\\nHere, we consider the same activation function Ïƒ for all layers. Now, the left hand side of the\\nequation can be interpreted as a ï¬nite diï¬€erence representation of a time derivative, where Ï„ [â„“] is\\nthe time step size and y[â„“], y[â„“âˆ’1] are the values attained at two neighboring points in time. This\\nrelation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is\\nalso possible to learn the time step size Ï„ [â„“] as an additional variable, [2].\\nLet us now introduce the diï¬€erent ResNet versions from the original papers, [17, 19].\\n4.1.\\nDiï¬€erent ResNet Versions\\nIn contrast to the simpliï¬ed ResNet layer version (9) that we introduced, original ResNet ar-\\nchitectures [17] consist of residual blocks, cf. Figure 28. Here, diï¬€erent layers are grouped\\ntogether into one residual block and then residual blocks are stacked to form a ResNet.\\ny[â„“âˆ’1]\\nWeights\\nBN\\nReLU\\nWeights\\nBN\\n+\\nReLU\\ny[â„“]', metadata={'chunk': 83}),\n",
              " Document(page_content='âˆ‚L\\nâˆ‚W [â„“] = âˆ‚L\\nâˆ‚y[L] Â·\\nâ„“+2\\nY\\nj=L\\nâˆ‚y[j]\\nâˆ‚y[jâˆ’1] Â· âˆ‚y[â„“+1]\\nâˆ‚W [â„“] .\\nIn the case that we consider a very deep network, i.e. large L, the product in the derivative\\ncan be problematic, [5, 14], especially if we take derivatives with respect to variables from early\\nlayers. Two cases may occur:\\n1. If\\nâˆ‚y[j]\\nâˆ‚y[jâˆ’1] < 1 for all j, the product, and hence the whole derivative, tends to zero for\\ngrowing L. This problem is referred to as vanishing gradient.\\n2. On the other hand, if\\nâˆ‚y[j]\\nâˆ‚y[jâˆ’1] > 1 for all j, the product, and hence the whole derivative,\\ntends to inï¬nity for growing L. This problem is referred to as exploding gradient.\\nResidual Networks (ResNets) have been developed in [17, 19] with the intention to solve the\\nvanishing gradient problem. Employing the same notation as in FNNs, simpliï¬ed ResNet layers\\ncan be represented in the following way\\ny[â„“] = y[â„“âˆ’1] + Ïƒ[â„“](W [â„“âˆ’1]y[â„“âˆ’1] + b[â„“âˆ’1])\\nfor â„“= 1, . . . , L,\\n(9)', metadata={'chunk': 79}),\n",
              " Document(page_content='and\\nL (Î¸) = L (y[3](Î¸)).\\nWe deï¬ne for â„“= 1, . . . , L\\na[â„“] := Ïƒ[â„“](W [â„“âˆ’1]y[â„“âˆ’1]),\\n4\\nRESNET\\n37\\ny[0]\\ny[1]\\ny[2]\\ny[3]\\ninput\\nlayer\\nhidden layers\\nL\\noutput layer\\nW [0]\\nW [1]\\nW [2]\\nFigure 27. A ResNet with 2 hidden layers, one node per layer and depth L = 3.\\nso that in the ResNet setup\\ny[â„“] = y[â„“âˆ’1] + a[â„“].\\nComputing the components of the gradient, we employ the chain rule to obtain e.g.\\nâˆ‚L\\nâˆ‚W [0] = âˆ‚L\\nâˆ‚y[3] Â· âˆ‚y[3]\\nâˆ‚W [0]\\n= âˆ‚L\\nâˆ‚y[3] Â·\\nâˆ‚\\nâˆ‚W [0] (y[2] + a[3])\\n= âˆ‚L\\nâˆ‚y[3] Â·\\n\\x12 âˆ‚y[2]\\nâˆ‚W [0] + âˆ‚a[3]\\nâˆ‚y[2] Â· âˆ‚y[2]\\nâˆ‚W [0]\\n\\x13\\n= âˆ‚L\\nâˆ‚y[3] Â·\\n\\x12\\nI + âˆ‚a[3]\\nâˆ‚y[2]\\n\\x13\\nÂ· âˆ‚y[2]\\nâˆ‚W [0]\\n= âˆ‚L\\nâˆ‚y[3] Â·\\n\\x12\\nI + âˆ‚a[3]\\nâˆ‚y[2]\\n\\x13\\nÂ·\\n\\x12\\nI + âˆ‚a[2]\\nâˆ‚y[1]\\n\\x13\\nÂ· âˆ‚y[1]\\nâˆ‚W [0] ,\\nwhere I denotes the identity. In general for depth L, we get\\nâˆ‚L\\nâˆ‚W [â„“] = âˆ‚L\\nâˆ‚y[L] Â·\\nâ„“+2\\nY\\nj=L\\n\\x12\\nI +\\nâˆ‚a[j]\\nâˆ‚y[jâˆ’1]\\n\\x13\\nÂ· âˆ‚y[â„“+1]\\nâˆ‚W [â„“] .\\n(10)\\nIf we generalize the derivative (10) to ResNet architectures, where we do not only consider weights\\nW [â„“], see e.g. [2, Theorem 6.1], the structure of the product in the derivative remains the same,', metadata={'chunk': 81})]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval based Question/Answering Chain\n",
        "\n",
        "We will demonstrate using three LangChain retrieval Q&A chains:\n",
        "\n",
        "- `RetrievalQA`\n",
        "- `ConversationalRetrievalChain`\n",
        "- Advanced: customized Q&A prompt and format\n",
        "\n",
        "We begin by initializing a Vertex AI LLM and a LangChain retriever to fetch documents from our Chroma Database containing ingested pdfs of papers we fetched earlier.\n",
        "\n",
        "For Q&A chains our retriever is passed directly to the chain and can be used automatically without any further configuration.\n",
        "\n",
        "Behind the scenes, first the search query is passed to the retriever which runs a search and returns relevant document chunks.\n",
        "\n",
        "These chunks are then passed to the prompt used by the LLM to be used as context."
      ],
      "metadata": {
        "id": "C9NEgcimmk2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = VertexAI(model_name=\"gemini-pro\")\n",
        "\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "IET1BDbmfWRd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `RetrievalQA` chain\n",
        "\n",
        "This is the simplest document Q&A chain offered by LangChain.\n",
        "\n",
        "There are several different chain types available.\n",
        "\n",
        "- In these examples we use the `stuff` type, which simply inserts all of the document chunks into the prompt.\n",
        "- This has the advantage of only making a single LLM call, which is faster and more cost efficient.\n",
        "- However, if we have a large number of search results we run the risk of exceeding the token limit in our prompt, or truncating useful information.\n",
        "- Other chain types such as `map_reduce` and `refine` use an iterative process which makes multiple LLM calls, taking individual document chunks at a time and refining the answer iteratively."
      ],
      "metadata": {
        "id": "8F8qhv5dnSbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                           chain_type=\"stuff\",\n",
        "                                           retriever=retriever)\n",
        "\n",
        "retrieval_qa.invoke(search_query)"
      ],
      "metadata": {
        "id": "SAugZ8D8oHPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inspecting the process\n",
        "\n",
        "If we add `return_source_documents=True` we can inspect the document chunks that were returned by the retriever.\n",
        "\n",
        "This is helpful for debugging, as these chunks may not always be relevant to the answer, or their relevance might not be obvious."
      ],
      "metadata": {
        "id": "dVYKSS3QoHlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=retriever,\n",
        "                                 return_source_documents=True)\n",
        "\n",
        "results = retrieval_qa.invoke(search_query)\n",
        "\n",
        "print(\"*\" * 79)\n",
        "print(results[\"result\"])\n",
        "print(\"*\" * 79)\n",
        "for doc in results[\"source_documents\"]:\n",
        "    print(\"-\" * 79)\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H-UoBsfhfjr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationalRetrievalChain\n",
        "`ConversationalRetrievalChain` remembers and uses previous questions so you can have a chat-like discovery process.\n",
        "\n",
        "To use this chain we must provide a memory class to store and pass the previous messages to the LLM as context. Here we use the `ConversationBufferMemory` class that comes with LangChain."
      ],
      "metadata": {
        "id": "bqNFe4A8hBhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversational_retrieval = ConversationalRetrievalChain.from_llm(llm=llm,\n",
        "                                                                 retriever=retriever,\n",
        "                                                                 memory=memory)\n",
        "\n",
        "\n",
        "conversational_retrieval.invoke(search_query)[\"answer\"]"
      ],
      "metadata": {
        "id": "VTzwlThrCcrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_query = \"What about other activation functions?\"\n",
        "result = conversational_retrieval.invoke(new_query)\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "id": "zVqxz1Wah9X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_query = \"give me specifically for sigmoid\"\n",
        "result = conversational_retrieval.invoke(new_query)\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "id": "exXwDy-9iS4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced: Modifying the default langchain prompt\n",
        "\n",
        "In all of the previous examples we used the default prompt that comes with Langchain.\n",
        "\n",
        "We can inspect our chain object to discover the wording of the prompt template being used.\n",
        "\n",
        "We may find that this is not suitable for our purposes, and we may wish to customise the prompt, for example to present our results in a different format, or to specify additional constraints."
      ],
      "metadata": {
        "id": "2xYMUCu_iiag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
        ")\n",
        "\n",
        "print(qa.combine_documents_chain.llm_chain.prompt.template)"
      ],
      "metadata": {
        "id": "6z4RGovgismU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's modify the prompt to return an answer in a single word (useful for yes/no questions). We will constrain the LLM to say 'I don't know' if it cannot answer.\n",
        "\n",
        "We create a new prompt_template and pass this in using the template argument."
      ],
      "metadata": {
        "id": "Oq_sKCjwi2zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"SYSTEM: You are an intelligent research assistant helping the users with their research paper questions.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
        "\n",
        "Do not try to make up an answer:\n",
        " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
        " - If the context is empty, just say \"I do not know the answer to that.\"\n",
        "\n",
        "=============\n",
        "{context}\n",
        "=============\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "WVEBD5kviv99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also customize the retriever"
      ],
      "metadata": {
        "id": "13V3elQKjThm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title No. of Results { display-mode: \"form\" }\n",
        "NUMBER_OF_RESULTS = \"3\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "LdzJI_QxEuKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create chain to answer questions\n",
        "\n",
        "# Expose index to the retriever\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\n",
        "        \"k\": int(NUMBER_OF_RESULTS)\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "SEk2IWCFjUyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    verbose=True,\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": prompt,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "67P3zYaQjhPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa.combine_documents_chain.llm_chain.prompt.template)"
      ],
      "metadata": {
        "id": "oCXnI6Z4jtzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable for troubleshooting\n",
        "qa.combine_documents_chain.verbose = True\n",
        "qa.combine_documents_chain.llm_chain.verbose = True\n",
        "qa.combine_documents_chain.llm_chain.llm.verbose = True\n",
        "\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def formatter(result):\n",
        "    print(f\"Query: {result['query']}\")\n",
        "    print(\".\" * 80)\n",
        "    print(f\"Response: {wrap(result['result'])}\")\n",
        "    print(\".\" * 80)\n",
        "    if \"source_documents\" in result.keys():\n",
        "        for idx, ref in enumerate(result[\"source_documents\"]):\n",
        "            print(\"-\" * 80)\n",
        "            print(f\"REFERENCE #{idx}\")\n",
        "            print(\"-\" * 80)\n",
        "            if \"score\" in ref.metadata:\n",
        "                print(f\"Matching Score: {ref.metadata['score']}\")\n",
        "            if \"source\" in ref.metadata:\n",
        "                print(f\"Document Source: {ref.metadata['source']}\")\n",
        "            if \"document_name\" in ref.metadata:\n",
        "                print(f\"Document Name: {ref.metadata['document_name']}\")\n",
        "            print(\".\" * 80)\n",
        "            print(f\"Content: \\n{wrap(ref.page_content)}\")\n",
        "    print(\".\" * 80)\n",
        "\n",
        "\n",
        "def wrap(s):\n",
        "    return \"\\n\".join(textwrap.wrap(s, width=120, break_long_words=False))\n",
        "\n",
        "\n",
        "def ask(query, qa=qa, k=NUMBER_OF_RESULTS):\n",
        "    result = qa({\"query\": query})\n",
        "    return formatter(result)"
      ],
      "metadata": {
        "id": "J0YbNsIukLT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask(query=search_query)"
      ],
      "metadata": {
        "id": "Z18z8B49kdQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}