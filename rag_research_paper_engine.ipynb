{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Research Paper Engine using arXiv, LangChain 🦜️🔗 and Google Gemini"
      ],
      "metadata": {
        "id": "PetNuzxJZwow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Tahreem Rasul](https://github.com/tahreemrasul) |"
      ],
      "metadata": {
        "id": "xvIyW-EMukRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/tahreemrasul/rag_research_paper_engine_workshop/blob/main/rag_research_paper_engine.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/tahreemrasul/rag_research_paper_engine_workshop/blob/main/rag_research_paper_engine.ipynb\">\n",
        "      <img width=\"28px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "w82RyFPUG4W-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates implementing a research paper engine using the arXiv API to show how to improve LLM's response by augmenting LLM's knowledge with external data sources such as documents. The notebooks uses Vertex AI Gemini Pro 1.0 for Text, Embeddings for Text API, arXiv API and LangChain 🦜️🔗.\n",
        "\n",
        "## Context\n",
        "\n",
        "Large Language Models (LLMs) have improved quantitatively and qualitatively. They can learn new abilities without being directly trained on them. However, there are constraints with LLMs - they are unaware of events after training and it is almost impossible to trace the sources to their responses. It is preferred for LLM based systems to cite their sources and be grounded in facts.\n",
        "\n",
        "To solve for the constraints, one of the approaches is to augment the prompt sent to LLM with relevant data retrieved from an external knowledge base through Information Retrieval (IR) mechanism.\n",
        "\n",
        "This approach is called Retrieval Augmented Generation (RAG), also known as Generative QA in the context of a Question Answering task. There are two main components in RAG based architecture: (1) Retriever and (2) Generator."
      ],
      "metadata": {
        "id": "S-KL6H18sXA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "### Install Vertex AI SDK, other packages and their dependencies\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ],
      "metadata": {
        "id": "9dVwHu5gs-dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LangChain and related packages\n",
        "!pip install --upgrade --quiet langchain langchain-google-vertexai langchain-community chromadb arxiv pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqZuoY_dZ7G8",
        "outputId": "e6fd9623-e204-43da-9c30-1d9fac9659d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.0/122.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ],
      "metadata": {
        "id": "SKByjcaPtDHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgtqfYu9jf7o",
        "outputId": "7b108518-b4d1-4a46-c3ba-47719819fdb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ Before proceeding, please wait for the kernel to finish restarting ⚠️</b>\n",
        "</div>"
      ],
      "metadata": {
        "id": "v2cbF6nptJu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticating your notebook environment\n",
        "\n",
        "If you are using Colab, you will need to authenticate yourself first. The next cell will check if you are currently using Colab, and will start the authentication process.\n",
        "\n",
        "If you are using Vertex AI Workbench, you will not require additional authentication.\n",
        "\n",
        "For more information, you can check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
      ],
      "metadata": {
        "id": "V2-PTrJjuHVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "vPo0ch83ounD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Relevant Papers from arXiv API\n",
        "\n",
        "This step retrieves relevant research papers based on the user query. The document corpus used as dataset will be the research papers pulled from the `arXiv` API. We will be using the `ArxivLoader` class from LangChain to load the PDFs of these papers."
      ],
      "metadata": {
        "id": "wFJUl7RhpYdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query & No. of Papers { display-mode: \"form\" }\n",
        "query = \"neural networks\"  # @param {type:\"string\"}\n",
        "\n",
        "# @title Total Docs { display-mode: \"form\" }\n",
        "num_papers = \"3\"  # @param {type: \"string\"}"
      ],
      "metadata": {
        "id": "JsTU3RZkx7dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import ArxivLoader\n",
        "\n",
        "arxiv_docs = ArxivLoader(query=query, load_max_docs=int(num_papers)).load()"
      ],
      "metadata": {
        "id": "tOBHwVySxepN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once retreived, display the metadata to check which papers were returned"
      ],
      "metadata": {
        "id": "2661AqkeqwlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(int(num_papers)):\n",
        "  print(f\"Paper # {i+1}:\")\n",
        "  print(f\"Published: {arxiv_docs[i].metadata['Published']}\")\n",
        "  print(f\"Title: {arxiv_docs[i].metadata['Title']}\")\n",
        "  print(f\"Authors: {arxiv_docs[i].metadata['Authors']}\")\n",
        "  print(f\"Summary: {arxiv_docs[i].metadata['Summary']}\")\n",
        "  print('------------------------------------------------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPOwT8JRy3Ww",
        "outputId": "f9da04ed-845d-418a-bc95-9c3c7237a83c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper # 1:\n",
            "Published: 2023-04-18\n",
            "Title: Lecture Notes: Neural Network Architectures\n",
            "Authors: Evelyn Herberg\n",
            "Summary: These lecture notes provide an overview of Neural Network architectures from\n",
            "a mathematical point of view. Especially, Machine Learning with Neural Networks\n",
            "is seen as an optimization problem. Covered are an introduction to Neural\n",
            "Networks and the following architectures: Feedforward Neural Network,\n",
            "Convolutional Neural Network, ResNet, and Recurrent Neural Network.\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "Paper # 2:\n",
            "Published: 2023-11-18\n",
            "Title: Bayesian Neural Networks: A Min-Max Game Framework\n",
            "Authors: Junping Hong, Ercan Engin Kuruoglu\n",
            "Summary: Bayesian neural networks use random variables to describe the neural networks\n",
            "rather than deterministic neural networks and are mostly trained by variational\n",
            "inference which updates the mean and variance at the same time. Here, we\n",
            "formulate the Bayesian neural networks as a minimax game problem. We do the\n",
            "experiments on the MNIST data set and the primary result is comparable to the\n",
            "existing closed-loop transcription neural network. Finally, we reveal the\n",
            "connections between Bayesian neural networks and closed-loop transcription\n",
            "neural networks, and show our framework is rather practical, and provide\n",
            "another view of Bayesian neural networks.\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "Paper # 3:\n",
            "Published: 2005-04-13\n",
            "Title: Self-Organizing Multilayered Neural Networks of Optimal Complexity\n",
            "Authors: V. Schetinin\n",
            "Summary: The principles of self-organizing the neural networks of optimal complexity\n",
            "is considered under the unrepresentative learning set. The method of\n",
            "self-organizing the multi-layered neural networks is offered and used to train\n",
            "the logical neural networks which were applied to the medical diagnostics.\n",
            "------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunk documents - TextSplitter\n",
        "\n",
        "Split the documents retrieved into smaller chunks. When splitting the document, ensure a few chunks can fit within the context length of LLM."
      ],
      "metadata": {
        "id": "mCBqPg7hq0ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "pdf_data = []\n",
        "for doc in arxiv_docs:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    doc_splits = text_splitter.create_documents([doc.page_content])\n",
        "    for idx, split in enumerate(doc_splits):\n",
        "      split.metadata[\"chunk\"] = idx\n",
        "    pdf_data.append(doc_splits)\n",
        "\n",
        "print(f\"# of pdfs = {len(pdf_data)} \\n# of split documents = {sum([len(doc_splits) for doc_splits in pdf_data])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5IDeM1g_lpc",
        "outputId": "8b02596f-7431-400e-df0c-7616b82b062f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of pdfs = 3 \n",
            "# of split documents = 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the VertexAI Embedding model"
      ],
      "metadata": {
        "id": "w21N8p80rqKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Project { display-mode: \"form\" }\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "\n",
        "# @title Region { display-mode: \"form\" }\n",
        "REGION = \"\"  # @param {type: \"string\"}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf8_Q7mApLmM",
        "outputId": "805882e6-63bc-4c9f-dce8-53daa808b582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "embedding_model = VertexAIEmbeddings(\n",
        "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
        ")\n",
        "print(embedding_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68qVOQKyuQoT",
        "outputId": "bc112363-14bc-4338-b4f0-fc83b6c61579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=<vertexai.language_models.TextEmbeddingModel object at 0x7ad756133dc0> async_client=None project='research-paper-engine' location='us-central1' request_parallelism=5 max_retries=6 stop=None model_name='textembedding-gecko@latest' model_family=None full_model_name=None client_preview=None temperature=None max_output_tokens=None top_p=None top_k=None credentials=None n=1 streaming=False safety_settings=None api_transport=None api_endpoint=None tuned_model_name=None instance={'max_batch_size': 250, 'batch_size': 250, 'min_batch_size': 5, 'min_good_batch_size': 5, 'lock': <unlocked _thread.lock object at 0x7ad757b3af00>, 'batch_size_validated': False, 'task_executor': <concurrent.futures.thread.ThreadPoolExecutor object at 0x7ad763cfec80>, 'embeddings_task_type_supported': True, 'get_embeddings_with_retry': <function _TextEmbeddingModel.get_embeddings at 0x7ad756157520>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `ChromaDB` as Vector Store\n",
        "\n",
        "This step generates embeddings from the documents and adds the embeddings to the vector store. The vector store being used is the `Chroma` database.\n"
      ],
      "metadata": {
        "id": "I_5nT5YVozTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "db = Chroma.from_documents(pdf_data[0], embedding_model)"
      ],
      "metadata": {
        "id": "lqyY4z2STxNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title search query { display-mode: \"form\" }\n",
        "search_query = \"What should be considered when taking derivatives of ReLU?\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "skgUgy7kga3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify the `ChromaDB` with similarity search"
      ],
      "metadata": {
        "id": "r_fuacRXpPV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db.similarity_search(\n",
        "    search_query\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S48Gk44We3rI",
        "outputId": "1e722d76-f0e8-432f-85be-9546a2dd3fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='identity function often helps speed up convergence, since it resembles a linear model, as long as\\nthe values are close to zero. Another challenge that needs to be overcome is vanishing derivatives,\\nwhich is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not\\nbounded on positive values, while also being comparatively cheap to compute, because linear\\ncomputations tend to be very well optimized in modern computing. Altogether, these advan-\\ntages have resulted in ReLU (and variants thereof) becoming the most widely used activation\\nfunction currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was\\nintroduced. When taking derivatives of ReLU one needs to account for the non-diﬀerentiability\\nat 0, but in numerical practice this is easily overcome.\\nWith the help of Neural Networks we want to solve a task, cf.\\n[15, Section 5.1].\\nLet the\\nperformance of the algorithm for the given task be measured by the loss function L, which', metadata={'chunk': 8}),\n",
              " Document(page_content='⇒\\ny[ℓ] −y[ℓ−1]\\nτ [ℓ]\\n= σ(W [ℓ−1]y[ℓ−1] + b[ℓ−1]).\\n4\\nRESNET\\n38\\nHere, we consider the same activation function σ for all layers. Now, the left hand side of the\\nequation can be interpreted as a ﬁnite diﬀerence representation of a time derivative, where τ [ℓ] is\\nthe time step size and y[ℓ], y[ℓ−1] are the values attained at two neighboring points in time. This\\nrelation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is\\nalso possible to learn the time step size τ [ℓ] as an additional variable, [2].\\nLet us now introduce the diﬀerent ResNet versions from the original papers, [17, 19].\\n4.1.\\nDiﬀerent ResNet Versions\\nIn contrast to the simpliﬁed ResNet layer version (9) that we introduced, original ResNet ar-\\nchitectures [17] consist of residual blocks, cf. Figure 28. Here, diﬀerent layers are grouped\\ntogether into one residual block and then residual blocks are stacked to form a ResNet.\\ny[ℓ−1]\\nWeights\\nBN\\nReLU\\nWeights\\nBN\\n+\\nReLU\\ny[ℓ]', metadata={'chunk': 83}),\n",
              " Document(page_content='∂L\\n∂W [ℓ] = ∂L\\n∂y[L] ·\\nℓ+2\\nY\\nj=L\\n∂y[j]\\n∂y[j−1] · ∂y[ℓ+1]\\n∂W [ℓ] .\\nIn the case that we consider a very deep network, i.e. large L, the product in the derivative\\ncan be problematic, [5, 14], especially if we take derivatives with respect to variables from early\\nlayers. Two cases may occur:\\n1. If\\n∂y[j]\\n∂y[j−1] < 1 for all j, the product, and hence the whole derivative, tends to zero for\\ngrowing L. This problem is referred to as vanishing gradient.\\n2. On the other hand, if\\n∂y[j]\\n∂y[j−1] > 1 for all j, the product, and hence the whole derivative,\\ntends to inﬁnity for growing L. This problem is referred to as exploding gradient.\\nResidual Networks (ResNets) have been developed in [17, 19] with the intention to solve the\\nvanishing gradient problem. Employing the same notation as in FNNs, simpliﬁed ResNet layers\\ncan be represented in the following way\\ny[ℓ] = y[ℓ−1] + σ[ℓ](W [ℓ−1]y[ℓ−1] + b[ℓ−1])\\nfor ℓ= 1, . . . , L,\\n(9)', metadata={'chunk': 79}),\n",
              " Document(page_content='and\\nL (θ) = L (y[3](θ)).\\nWe deﬁne for ℓ= 1, . . . , L\\na[ℓ] := σ[ℓ](W [ℓ−1]y[ℓ−1]),\\n4\\nRESNET\\n37\\ny[0]\\ny[1]\\ny[2]\\ny[3]\\ninput\\nlayer\\nhidden layers\\nL\\noutput layer\\nW [0]\\nW [1]\\nW [2]\\nFigure 27. A ResNet with 2 hidden layers, one node per layer and depth L = 3.\\nso that in the ResNet setup\\ny[ℓ] = y[ℓ−1] + a[ℓ].\\nComputing the components of the gradient, we employ the chain rule to obtain e.g.\\n∂L\\n∂W [0] = ∂L\\n∂y[3] · ∂y[3]\\n∂W [0]\\n= ∂L\\n∂y[3] ·\\n∂\\n∂W [0] (y[2] + a[3])\\n= ∂L\\n∂y[3] ·\\n\\x12 ∂y[2]\\n∂W [0] + ∂a[3]\\n∂y[2] · ∂y[2]\\n∂W [0]\\n\\x13\\n= ∂L\\n∂y[3] ·\\n\\x12\\nI + ∂a[3]\\n∂y[2]\\n\\x13\\n· ∂y[2]\\n∂W [0]\\n= ∂L\\n∂y[3] ·\\n\\x12\\nI + ∂a[3]\\n∂y[2]\\n\\x13\\n·\\n\\x12\\nI + ∂a[2]\\n∂y[1]\\n\\x13\\n· ∂y[1]\\n∂W [0] ,\\nwhere I denotes the identity. In general for depth L, we get\\n∂L\\n∂W [ℓ] = ∂L\\n∂y[L] ·\\nℓ+2\\nY\\nj=L\\n\\x12\\nI +\\n∂a[j]\\n∂y[j−1]\\n\\x13\\n· ∂y[ℓ+1]\\n∂W [ℓ] .\\n(10)\\nIf we generalize the derivative (10) to ResNet architectures, where we do not only consider weights\\nW [ℓ], see e.g. [2, Theorem 6.1], the structure of the product in the derivative remains the same,', metadata={'chunk': 81})]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval based Question/Answering Chain\n",
        "\n",
        "We will demonstrate using three LangChain retrieval Q&A chains:\n",
        "\n",
        "- `RetrievalQA`\n",
        "- `ConversationalRetrievalChain`\n",
        "- Advanced: customized Q&A prompt and format\n",
        "\n",
        "We begin by initializing a Vertex AI LLM and a LangChain retriever to fetch documents from our Chroma Database containing ingested pdfs of papers we fetched earlier.\n",
        "\n",
        "For Q&A chains our retriever is passed directly to the chain and can be used automatically without any further configuration.\n",
        "\n",
        "Behind the scenes, first the search query is passed to the retriever which runs a search and returns relevant document chunks.\n",
        "\n",
        "These chunks are then passed to the prompt used by the LLM to be used as context."
      ],
      "metadata": {
        "id": "C9NEgcimmk2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = VertexAI(model_name=\"gemini-pro\")\n",
        "\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "IET1BDbmfWRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `RetrievalQA` chain\n",
        "\n",
        "This is the simplest document Q&A chain offered by LangChain.\n",
        "\n",
        "There are several different chain types available.\n",
        "\n",
        "- In these examples we use the `stuff` type, which simply inserts all of the document chunks into the prompt.\n",
        "- This has the advantage of only making a single LLM call, which is faster and more cost efficient.\n",
        "- However, if we have a large number of search results we run the risk of exceeding the token limit in our prompt, or truncating useful information.\n",
        "- Other chain types such as `map_reduce` and `refine` use an iterative process which makes multiple LLM calls, taking individual document chunks at a time and refining the answer iteratively."
      ],
      "metadata": {
        "id": "8F8qhv5dnSbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                           chain_type=\"stuff\",\n",
        "                                           retriever=retriever)\n",
        "\n",
        "retrieval_qa.invoke(search_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAugZ8D8oHPK",
        "outputId": "28186b8d-6d1b-4aad-ff9f-059b1a2ac383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What should be considered when taking derivatives of ReLU?',\n",
              " 'result': 'When taking derivatives of ReLU, one needs to account for the non-differentiability at 0. However, this can be easily overcome in numerical practice.'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inspecting the process\n",
        "\n",
        "If we add `return_source_documents=True` we can inspect the document chunks that were returned by the retriever.\n",
        "\n",
        "This is helpful for debugging, as these chunks may not always be relevant to the answer, or their relevance might not be obvious."
      ],
      "metadata": {
        "id": "dVYKSS3QoHlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                 chain_type=\"stuff\",\n",
        "                                 retriever=retriever,\n",
        "                                 return_source_documents=True)\n",
        "\n",
        "results = retrieval_qa.invoke(search_query)\n",
        "\n",
        "print(\"*\" * 79)\n",
        "print(results[\"result\"])\n",
        "print(\"*\" * 79)\n",
        "for doc in results[\"source_documents\"]:\n",
        "    print(\"-\" * 79)\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H-UoBsfhfjr8",
        "outputId": "434007f2-8b7d-49c0-9165-a3e71e76d27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************************************\n",
            "The text you provided does not contain information about taking derivatives of ReLU. Therefore, I cannot answer your question. \n",
            "\n",
            "*******************************************************************************\n",
            "-------------------------------------------------------------------------------\n",
            "identity function often helps speed up convergence, since it resembles a linear model, as long as\n",
            "the values are close to zero. Another challenge that needs to be overcome is vanishing derivatives,\n",
            "which is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not\n",
            "bounded on positive values, while also being comparatively cheap to compute, because linear\n",
            "computations tend to be very well optimized in modern computing. Altogether, these advan-\n",
            "tages have resulted in ReLU (and variants thereof) becoming the most widely used activation\n",
            "function currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was\n",
            "introduced. When taking derivatives of ReLU one needs to account for the non-diﬀerentiability\n",
            "at 0, but in numerical practice this is easily overcome.\n",
            "With the help of Neural Networks we want to solve a task, cf.\n",
            "[15, Section 5.1].\n",
            "Let the\n",
            "performance of the algorithm for the given task be measured by the loss function L, which\n",
            "-------------------------------------------------------------------------------\n",
            "⇒\n",
            "y[ℓ] −y[ℓ−1]\n",
            "τ [ℓ]\n",
            "= σ(W [ℓ−1]y[ℓ−1] + b[ℓ−1]).\n",
            "4\n",
            "RESNET\n",
            "38\n",
            "Here, we consider the same activation function σ for all layers. Now, the left hand side of the\n",
            "equation can be interpreted as a ﬁnite diﬀerence representation of a time derivative, where τ [ℓ] is\n",
            "the time step size and y[ℓ], y[ℓ−1] are the values attained at two neighboring points in time. This\n",
            "relation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is\n",
            "also possible to learn the time step size τ [ℓ] as an additional variable, [2].\n",
            "Let us now introduce the diﬀerent ResNet versions from the original papers, [17, 19].\n",
            "4.1.\n",
            "Diﬀerent ResNet Versions\n",
            "In contrast to the simpliﬁed ResNet layer version (9) that we introduced, original ResNet ar-\n",
            "chitectures [17] consist of residual blocks, cf. Figure 28. Here, diﬀerent layers are grouped\n",
            "together into one residual block and then residual blocks are stacked to form a ResNet.\n",
            "y[ℓ−1]\n",
            "Weights\n",
            "BN\n",
            "ReLU\n",
            "Weights\n",
            "BN\n",
            "+\n",
            "ReLU\n",
            "y[ℓ]\n",
            "-------------------------------------------------------------------------------\n",
            "∂L\n",
            "∂W [ℓ] = ∂L\n",
            "∂y[L] ·\n",
            "ℓ+2\n",
            "Y\n",
            "j=L\n",
            "∂y[j]\n",
            "∂y[j−1] · ∂y[ℓ+1]\n",
            "∂W [ℓ] .\n",
            "In the case that we consider a very deep network, i.e. large L, the product in the derivative\n",
            "can be problematic, [5, 14], especially if we take derivatives with respect to variables from early\n",
            "layers. Two cases may occur:\n",
            "1. If\n",
            "∂y[j]\n",
            "∂y[j−1] < 1 for all j, the product, and hence the whole derivative, tends to zero for\n",
            "growing L. This problem is referred to as vanishing gradient.\n",
            "2. On the other hand, if\n",
            "∂y[j]\n",
            "∂y[j−1] > 1 for all j, the product, and hence the whole derivative,\n",
            "tends to inﬁnity for growing L. This problem is referred to as exploding gradient.\n",
            "Residual Networks (ResNets) have been developed in [17, 19] with the intention to solve the\n",
            "vanishing gradient problem. Employing the same notation as in FNNs, simpliﬁed ResNet layers\n",
            "can be represented in the following way\n",
            "y[ℓ] = y[ℓ−1] + σ[ℓ](W [ℓ−1]y[ℓ−1] + b[ℓ−1])\n",
            "for ℓ= 1, . . . , L,\n",
            "(9)\n",
            "-------------------------------------------------------------------------------\n",
            "and\n",
            "L (θ) = L (y[3](θ)).\n",
            "We deﬁne for ℓ= 1, . . . , L\n",
            "a[ℓ] := σ[ℓ](W [ℓ−1]y[ℓ−1]),\n",
            "4\n",
            "RESNET\n",
            "37\n",
            "y[0]\n",
            "y[1]\n",
            "y[2]\n",
            "y[3]\n",
            "input\n",
            "layer\n",
            "hidden layers\n",
            "L\n",
            "output layer\n",
            "W [0]\n",
            "W [1]\n",
            "W [2]\n",
            "Figure 27. A ResNet with 2 hidden layers, one node per layer and depth L = 3.\n",
            "so that in the ResNet setup\n",
            "y[ℓ] = y[ℓ−1] + a[ℓ].\n",
            "Computing the components of the gradient, we employ the chain rule to obtain e.g.\n",
            "∂L\n",
            "∂W [0] = ∂L\n",
            "∂y[3] · ∂y[3]\n",
            "∂W [0]\n",
            "= ∂L\n",
            "∂y[3] ·\n",
            "∂\n",
            "∂W [0] (y[2] + a[3])\n",
            "= ∂L\n",
            "∂y[3] ·\n",
            "\u0012 ∂y[2]\n",
            "∂W [0] + ∂a[3]\n",
            "∂y[2] · ∂y[2]\n",
            "∂W [0]\n",
            "\u0013\n",
            "= ∂L\n",
            "∂y[3] ·\n",
            "\u0012\n",
            "I + ∂a[3]\n",
            "∂y[2]\n",
            "\u0013\n",
            "· ∂y[2]\n",
            "∂W [0]\n",
            "= ∂L\n",
            "∂y[3] ·\n",
            "\u0012\n",
            "I + ∂a[3]\n",
            "∂y[2]\n",
            "\u0013\n",
            "·\n",
            "\u0012\n",
            "I + ∂a[2]\n",
            "∂y[1]\n",
            "\u0013\n",
            "· ∂y[1]\n",
            "∂W [0] ,\n",
            "where I denotes the identity. In general for depth L, we get\n",
            "∂L\n",
            "∂W [ℓ] = ∂L\n",
            "∂y[L] ·\n",
            "ℓ+2\n",
            "Y\n",
            "j=L\n",
            "\u0012\n",
            "I +\n",
            "∂a[j]\n",
            "∂y[j−1]\n",
            "\u0013\n",
            "· ∂y[ℓ+1]\n",
            "∂W [ℓ] .\n",
            "(10)\n",
            "If we generalize the derivative (10) to ResNet architectures, where we do not only consider weights\n",
            "W [ℓ], see e.g. [2, Theorem 6.1], the structure of the product in the derivative remains the same,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationalRetrievalChain\n",
        "`ConversationalRetrievalChain` remembers and uses previous questions so you can have a chat-like discovery process.\n",
        "\n",
        "To use this chain we must provide a memory class to store and pass the previous messages to the LLM as context. Here we use the `ConversationBufferMemory` class that comes with LangChain."
      ],
      "metadata": {
        "id": "bqNFe4A8hBhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversational_retrieval = ConversationalRetrievalChain.from_llm(llm=llm,\n",
        "                                                                 retriever=retriever,\n",
        "                                                                 memory=memory)\n",
        "\n",
        "\n",
        "conversational_retrieval.invoke(search_query)[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "VTzwlThrCcrF",
        "outputId": "3be028f8-a117-4af6-bc28-2276fe184b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'When taking derivatives of ReLU, the non-differentiability at 0 needs to be accounted for. However, this can be easily overcome in numerical practice. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_query = \"What about other activation functions?\"\n",
        "result = conversational_retrieval.invoke(new_query)\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVqxz1Wah9X3",
        "outputId": "ce16d9db-eb4c-4ba3-f2d2-069646d47027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When taking derivatives of other activation functions, one needs to consider the following:\n",
            "\n",
            "* The specific form of the activation function. Different activation functions have different derivatives, so it is important to use the correct derivative for the activation function you are using.\n",
            "* The properties of the activation function. Some activation functions, such as the sigmoid function, have derivatives that are bounded between 0 and 1. This can make it difficult to train deep neural networks with these activation functions, as the gradients can become very small. Other activation functions, such as the ReLU function, have derivatives that are unbounded. This can make it easier to train deep neural networks with these activation functions, but it can also lead to instability.\n",
            "* The numerical stability of the derivative. Some activation functions, such as the sigmoid function, can have derivatives that are very sensitive to small changes in the input. This can make it difficult to train neural networks with these activation functions numerically. Other activation functions, such as the ReLU function, have derivatives that are more stable.\n",
            "\n",
            "In general, it is important to choose an activation function that is both appropriate for the task at hand and numerically stable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_query = \"give me specifically for sigmoid\"\n",
        "result = conversational_retrieval.invoke(new_query)\n",
        "print(result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exXwDy-9iS4w",
        "outputId": "a10228e9-d842-4f1e-cdf9-1e0c5a580d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Considerations for Derivative of Sigmoid Activation Function:\n",
            "\n",
            "Here are the specific considerations when taking the derivative of the sigmoid activation function:\n",
            "\n",
            "**Non-Differentiability at 0:**\n",
            "\n",
            "- The sigmoid function is not differentiable at 0. This means that the derivative is undefined at this point.\n",
            "- In practice, numerical methods are used to approximate the derivative around 0. These methods usually involve taking the limit of the derivative as the input approaches 0.\n",
            "\n",
            "**Boundedness:**\n",
            "\n",
            "- The derivative of the sigmoid function is always between 0 and 0.25. This means that the derivative can become very small for large or small input values.\n",
            "- This can lead to the \"vanishing gradient problem,\" where the gradient becomes too small to effectively train the network. \n",
            "\n",
            "**Computational Efficiency:**\n",
            "\n",
            "- The sigmoid function is relatively cheap to compute, but its derivative is slightly more expensive due to the exponential term.\n",
            "- However, modern computing systems are well-optimized for linear computations, making the overall computational cost manageable.\n",
            "\n",
            "**Alternatives:**\n",
            "\n",
            "- Due to the vanishing gradient problem, other activation functions like ReLU and Leaky ReLU are often preferred in practice. These functions have derivatives that are not bounded and do not suffer from the vanishing gradient problem.\n",
            "\n",
            "**In summary:**\n",
            "\n",
            "- The sigmoid activation function has a non-differentiable point at 0 and a bounded derivative. \n",
            "- These limitations can lead to difficulties during training, particularly the vanishing gradient problem. \n",
            "- Alternative activation functions are often preferred for their computational efficiency and better training characteristics. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingest PDF files\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# split the documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# Add chunk number to metadata\n",
        "for idx, split in enumerate(doc_splits):\n",
        "    split.metadata[\"chunk\"] = idx\n",
        "\n",
        "print(f\"# of documents = {len(doc_splits)}\")"
      ],
      "metadata": {
        "id": "58X14Y7E6hpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RFMkdr-Sie-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced: Modifying the default langchain prompt\n",
        "\n",
        "In all of the previous examples we used the default prompt that comes with Langchain.\n",
        "\n",
        "We can inspect our chain object to discover the wording of the prompt template being used.\n",
        "\n",
        "We may find that this is not suitable for our purposes, and we may wish to customise the prompt, for example to present our results in a different format, or to specify additional constraints."
      ],
      "metadata": {
        "id": "2xYMUCu_iiag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
        ")\n",
        "\n",
        "print(qa.combine_documents_chain.llm_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z4RGovgismU",
        "outputId": "17ba0320-be48-4e26-d27e-48b411132356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "{context}\n",
            "\n",
            "Question: {question}\n",
            "Helpful Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's modify the prompt to return an answer in a single word (useful for yes/no questions). We will constrain the LLM to say 'I don't know' if it cannot answer.\n",
        "\n",
        "We create a new prompt_template and pass this in using the template argument."
      ],
      "metadata": {
        "id": "Oq_sKCjwi2zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"SYSTEM: You are an intelligent research assistant helping the users with their research paper questions.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
        "\n",
        "Do not try to make up an answer:\n",
        " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
        " - If the context is empty, just say \"I do not know the answer to that.\"\n",
        "\n",
        "=============\n",
        "{context}\n",
        "=============\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "WVEBD5kviv99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also customize the retriever"
      ],
      "metadata": {
        "id": "13V3elQKjThm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title No. of Results { display-mode: \"form\" }\n",
        "NUMBER_OF_RESULTS = \"3\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "LdzJI_QxEuKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create chain to answer questions\n",
        "\n",
        "# Expose index to the retriever\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\n",
        "        \"k\": int(NUMBER_OF_RESULTS)\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "SEk2IWCFjUyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    verbose=True,\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": prompt,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "67P3zYaQjhPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa.combine_documents_chain.llm_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCXnI6Z4jtzE",
        "outputId": "9d3bbc70-93d2-4f22-c243-2a4f0a0d4a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: You are an intelligent research assistant helping the users with their research paper questions.\n",
            "\n",
            "Question: {question}\n",
            "\n",
            "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
            "\n",
            "Do not try to make up an answer:\n",
            " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
            " - If the context is empty, just say \"I do not know the answer to that.\"\n",
            "\n",
            "=============\n",
            "{context}\n",
            "=============\n",
            "\n",
            "Question: {question}\n",
            "Helpful Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable for troubleshooting\n",
        "qa.combine_documents_chain.verbose = True\n",
        "qa.combine_documents_chain.llm_chain.verbose = True\n",
        "qa.combine_documents_chain.llm_chain.llm.verbose = True\n",
        "\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def formatter(result):\n",
        "    print(f\"Query: {result['query']}\")\n",
        "    print(\".\" * 80)\n",
        "    print(f\"Response: {wrap(result['result'])}\")\n",
        "    print(\".\" * 80)\n",
        "    if \"source_documents\" in result.keys():\n",
        "        for idx, ref in enumerate(result[\"source_documents\"]):\n",
        "            print(\"-\" * 80)\n",
        "            print(f\"REFERENCE #{idx}\")\n",
        "            print(\"-\" * 80)\n",
        "            if \"score\" in ref.metadata:\n",
        "                print(f\"Matching Score: {ref.metadata['score']}\")\n",
        "            if \"source\" in ref.metadata:\n",
        "                print(f\"Document Source: {ref.metadata['source']}\")\n",
        "            if \"document_name\" in ref.metadata:\n",
        "                print(f\"Document Name: {ref.metadata['document_name']}\")\n",
        "            print(\".\" * 80)\n",
        "            print(f\"Content: \\n{wrap(ref.page_content)}\")\n",
        "    print(\".\" * 80)\n",
        "\n",
        "\n",
        "def wrap(s):\n",
        "    return \"\\n\".join(textwrap.wrap(s, width=120, break_long_words=False))\n",
        "\n",
        "\n",
        "def ask(query, qa=qa, k=NUMBER_OF_RESULTS, search_distance=SEARCH_DISTANCE_THRESHOLD):\n",
        "    # qa.retriever.search_kwargs[\"search_distance\"] = SEARCH_DISTANCE_THRESHOLD\n",
        "    # qa.retriever.search_kwargs[\"k\"] = NUMBER_OF_RESULTS\n",
        "    result = qa({\"query\": query})\n",
        "    return formatter(result)"
      ],
      "metadata": {
        "id": "J0YbNsIukLT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask(query=search_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z18z8B49kdQj",
        "outputId": "68ac4f0c-0b01-4cda-d463-3124d3cddbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent research assistant helping the users with their research paper questions.\n",
            "\n",
            "Question: What should be considered when taking derivatives of ReLU?\n",
            "\n",
            "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
            "\n",
            "Do not try to make up an answer:\n",
            " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
            " - If the context is empty, just say \"I do not know the answer to that.\"\n",
            "\n",
            "=============\n",
            "identity function often helps speed up convergence, since it resembles a linear model, as long as\n",
            "the values are close to zero. Another challenge that needs to be overcome is vanishing derivatives,\n",
            "which is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not\n",
            "bounded on positive values, while also being comparatively cheap to compute, because linear\n",
            "computations tend to be very well optimized in modern computing. Altogether, these advan-\n",
            "tages have resulted in ReLU (and variants thereof) becoming the most widely used activation\n",
            "function currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was\n",
            "introduced. When taking derivatives of ReLU one needs to account for the non-diﬀerentiability\n",
            "at 0, but in numerical practice this is easily overcome.\n",
            "With the help of Neural Networks we want to solve a task, cf.\n",
            "[15, Section 5.1].\n",
            "Let the\n",
            "performance of the algorithm for the given task be measured by the loss function L, which\n",
            "\n",
            "⇒\n",
            "y[ℓ] −y[ℓ−1]\n",
            "τ [ℓ]\n",
            "= σ(W [ℓ−1]y[ℓ−1] + b[ℓ−1]).\n",
            "4\n",
            "RESNET\n",
            "38\n",
            "Here, we consider the same activation function σ for all layers. Now, the left hand side of the\n",
            "equation can be interpreted as a ﬁnite diﬀerence representation of a time derivative, where τ [ℓ] is\n",
            "the time step size and y[ℓ], y[ℓ−1] are the values attained at two neighboring points in time. This\n",
            "relation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is\n",
            "also possible to learn the time step size τ [ℓ] as an additional variable, [2].\n",
            "Let us now introduce the diﬀerent ResNet versions from the original papers, [17, 19].\n",
            "4.1.\n",
            "Diﬀerent ResNet Versions\n",
            "In contrast to the simpliﬁed ResNet layer version (9) that we introduced, original ResNet ar-\n",
            "chitectures [17] consist of residual blocks, cf. Figure 28. Here, diﬀerent layers are grouped\n",
            "together into one residual block and then residual blocks are stacked to form a ResNet.\n",
            "y[ℓ−1]\n",
            "Weights\n",
            "BN\n",
            "ReLU\n",
            "Weights\n",
            "BN\n",
            "+\n",
            "ReLU\n",
            "y[ℓ]\n",
            "\n",
            "∂L\n",
            "∂W [ℓ] = ∂L\n",
            "∂y[L] ·\n",
            "ℓ+2\n",
            "Y\n",
            "j=L\n",
            "∂y[j]\n",
            "∂y[j−1] · ∂y[ℓ+1]\n",
            "∂W [ℓ] .\n",
            "In the case that we consider a very deep network, i.e. large L, the product in the derivative\n",
            "can be problematic, [5, 14], especially if we take derivatives with respect to variables from early\n",
            "layers. Two cases may occur:\n",
            "1. If\n",
            "∂y[j]\n",
            "∂y[j−1] < 1 for all j, the product, and hence the whole derivative, tends to zero for\n",
            "growing L. This problem is referred to as vanishing gradient.\n",
            "2. On the other hand, if\n",
            "∂y[j]\n",
            "∂y[j−1] > 1 for all j, the product, and hence the whole derivative,\n",
            "tends to inﬁnity for growing L. This problem is referred to as exploding gradient.\n",
            "Residual Networks (ResNets) have been developed in [17, 19] with the intention to solve the\n",
            "vanishing gradient problem. Employing the same notation as in FNNs, simpliﬁed ResNet layers\n",
            "can be represented in the following way\n",
            "y[ℓ] = y[ℓ−1] + σ[ℓ](W [ℓ−1]y[ℓ−1] + b[ℓ−1])\n",
            "for ℓ= 1, . . . , L,\n",
            "(9)\n",
            "=============\n",
            "\n",
            "Question: What should be considered when taking derivatives of ReLU?\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Query: What should be considered when taking derivatives of ReLU?\n",
            "................................................................................\n",
            "Response: When taking derivatives of ReLU, one needs to account for the non-differentiability at 0. However, in numerical\n",
            "practice, this is easily overcome.\n",
            "................................................................................\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #0\n",
            "--------------------------------------------------------------------------------\n",
            "................................................................................\n",
            "Content: \n",
            "identity function often helps speed up convergence, since it resembles a linear model, as long as the values are close\n",
            "to zero. Another challenge that needs to be overcome is vanishing derivatives, which is visibly present for Heaviside,\n",
            "sigmoid and hyperbolic tangent. In contrast, ReLU is not bounded on positive values, while also being comparatively\n",
            "cheap to compute, because linear computations tend to be very well optimized in modern computing. Altogether, these\n",
            "advan- tages have resulted in ReLU (and variants thereof) becoming the most widely used activation function currently.\n",
            "As a remedy for the vanishing gradient on negative values, leaky ReLU was introduced. When taking derivatives of ReLU\n",
            "one needs to account for the non-diﬀerentiability at 0, but in numerical practice this is easily overcome. With the help\n",
            "of Neural Networks we want to solve a task, cf. [15, Section 5.1]. Let the performance of the algorithm for the given\n",
            "task be measured by the loss function L, which\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #1\n",
            "--------------------------------------------------------------------------------\n",
            "................................................................................\n",
            "Content: \n",
            "⇒ y[ℓ] −y[ℓ−1] τ [ℓ] = σ(W [ℓ−1]y[ℓ−1] + b[ℓ−1]). 4 RESNET 38 Here, we consider the same activation function σ for all\n",
            "layers. Now, the left hand side of the equation can be interpreted as a ﬁnite diﬀerence representation of a time\n",
            "derivative, where τ [ℓ] is the time step size and y[ℓ], y[ℓ−1] are the values attained at two neighboring points in\n",
            "time. This relation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is also possible to\n",
            "learn the time step size τ [ℓ] as an additional variable, [2]. Let us now introduce the diﬀerent ResNet versions from\n",
            "the original papers, [17, 19]. 4.1. Diﬀerent ResNet Versions In contrast to the simpliﬁed ResNet layer version (9) that\n",
            "we introduced, original ResNet ar- chitectures [17] consist of residual blocks, cf. Figure 28. Here, diﬀerent layers are\n",
            "grouped together into one residual block and then residual blocks are stacked to form a ResNet. y[ℓ−1] Weights BN ReLU\n",
            "Weights BN + ReLU y[ℓ]\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #2\n",
            "--------------------------------------------------------------------------------\n",
            "................................................................................\n",
            "Content: \n",
            "∂L ∂W [ℓ] = ∂L ∂y[L] · ℓ+2 Y j=L ∂y[j] ∂y[j−1] · ∂y[ℓ+1] ∂W [ℓ] . In the case that we consider a very deep network, i.e.\n",
            "large L, the product in the derivative can be problematic, [5, 14], especially if we take derivatives with respect to\n",
            "variables from early layers. Two cases may occur: 1. If ∂y[j] ∂y[j−1] < 1 for all j, the product, and hence the whole\n",
            "derivative, tends to zero for growing L. This problem is referred to as vanishing gradient. 2. On the other hand, if\n",
            "∂y[j] ∂y[j−1] > 1 for all j, the product, and hence the whole derivative, tends to inﬁnity for growing L. This problem\n",
            "is referred to as exploding gradient. Residual Networks (ResNets) have been developed in [17, 19] with the intention to\n",
            "solve the vanishing gradient problem. Employing the same notation as in FNNs, simpliﬁed ResNet layers can be represented\n",
            "in the following way y[ℓ] = y[ℓ−1] + σ[ℓ](W [ℓ−1]y[ℓ−1] + b[ℓ−1]) for ℓ= 1, . . . , L, (9)\n",
            "................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2E-IHEE5kiEp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}